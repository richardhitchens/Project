{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from scipy.sparse import *\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "import pygraphviz\n",
    "import graphviz\n",
    "\n",
    "convert_zeroes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data from the file and load into a DataFrame\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # sort by VisitNumber to ensure everything is in the right order  \n",
    "    df = df.sort_values('VisitNumber')\n",
    "    df = df.reset_index()\n",
    "    # del df['index']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to transform data to create new features nd \n",
    "def feature_transformation(df, df_other, convert_zeroes=False):\n",
    "\n",
    "    # Replace nulls will 'Unknown' product categories\n",
    "    df['Upc'] = df['Upc'].fillna('UnknownUpc')\n",
    "    df['DepartmentDescription'] = df['DepartmentDescription'].fillna('UnknownDD')\n",
    "    df['FinelineNumber'] = df['FinelineNumber'].fillna('UnknownFN')\n",
    "\n",
    "    # Replace nulls will 'Unknown' product categories\n",
    "    df_other['Upc'] = df_other['Upc'].fillna('UnknownUpc')\n",
    "    df_other['DepartmentDescription'] = df_other['DepartmentDescription'].fillna('UnknownDD')\n",
    "    df_other['FinelineNumber'] = df_other['FinelineNumber'].fillna('UnknownFN')\n",
    "    \n",
    "    # Create a group of field headers that include categories from both the training and the test data sets\n",
    "    VisitNumber_u = list(sorted(df.VisitNumber.unique()))\n",
    "    VisitNumber_u_other = list(sorted(df_other.VisitNumber.unique()))\n",
    "\n",
    "    Upc_u = list(sorted(df.Upc.unique()))\n",
    "    Upc_u_other = list(sorted(df_other.Upc.unique()))\n",
    "    Upc_all = sorted(list(set(list(set(Upc_u)|set(Upc_u_other)))))\n",
    "\n",
    "    FN_u = list(sorted(df.FinelineNumber.unique()))\n",
    "    FN_u_other = list(sorted(df_other.FinelineNumber.unique()))\n",
    "    FN_all = sorted(list(set(FN_u)|set(FN_u_other)))\n",
    "\n",
    "    DD_u = list(sorted(df.DepartmentDescription.unique()))\n",
    "    DD_u_other = list(sorted(df_other.DepartmentDescription.unique()))\n",
    "    DD_all = sorted(list(set(DD_u)|set(DD_u_other)))\n",
    "    \n",
    "    # Convert Weekday text variable to numerical\n",
    "    df['Weekday'] = df['Weekday'].map({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
    " \n",
    "    # Create a return dummy\n",
    "    df['Return'] = np.where(df['ScanCount'] > 0, 0 , 1)\n",
    "\n",
    "    # Convert ScanCount negatives to 0's\n",
    "    if convert_zeroes:\n",
    "        df['ScanCount'] = np.where(df['ScanCount'] < 0 , 0, df['ScanCount'])\n",
    "\n",
    "    # Aggregate number of items by Department Description, i.e. sum ScanCount\n",
    "    dd = df.groupby(['VisitNumber','DepartmentDescription'], as_index=False)['ScanCount'].sum()\n",
    "    dd = dd.rename(columns={'ScanCount': 'ItemsDD'})\n",
    "    df = pd.merge(left=df, right=dd, on=['VisitNumber','DepartmentDescription'], how='left')\n",
    "\n",
    "    # Aggregate number of items by FinelineNumber, i.e. sum ScanCount\n",
    "    fn = df.groupby(['VisitNumber','FinelineNumber'], as_index=False)['ScanCount'].sum()\n",
    "    fn = fn.rename(columns={'ScanCount': 'ItemsFN'})\n",
    "    df = pd.merge(left=df, right=fn, on=['VisitNumber','FinelineNumber'], how='left')        \n",
    "\n",
    "    # Aggregate number of products by VisitNumber, i.e. count ScanCount\n",
    "    wd = df.groupby(['VisitNumber','Weekday'], as_index=False)['ScanCount'].count()\n",
    "    wd = wd.rename(columns={'ScanCount': 'NumProducts'})\n",
    "    Weekday_u = list(sorted(wd.Weekday.unique()))\n",
    "    df = pd.merge(left=df, right=wd, on=['VisitNumber','Weekday'], how='left')\n",
    "    \n",
    "    # Create a return dummy for each shopping visit\n",
    "    rt = df.groupby(['VisitNumber'], as_index=False)['Return'].sum()\n",
    "    rt['Return'] = np.where(rt['Return'] > 0 , 1, 0)\n",
    "\n",
    "    # Aggregate number of items by VisitNumber, i.e sum ScanCount\n",
    "    tt = df.groupby(['VisitNumber'], as_index=False)['ScanCount'].sum()\n",
    "    tt = tt.rename(columns={'ScanCount': 'NumItems'})\n",
    "    df = pd.merge(left=df, right=tt, on=['VisitNumber'], how='left')\n",
    "    \n",
    "    # Combine aggregates and sort by VisitNumber to get ordered row names of VisitNumber\n",
    "    tt['NumProducts'] = wd.NumProducts\n",
    "    tt['Return'] = rt.Return\n",
    "    tt.sort_values('VisitNumber')\n",
    "    aggs = tt[['NumItems', 'NumProducts', 'Return']] \n",
    "\n",
    "    # Isolate Visit Numbers\n",
    "    visit_numbers = tt.VisitNumber\n",
    "\n",
    "    # Create a sparse matrix of Weekday dummies for VisitNumber\n",
    "    data = wd['Weekday'].tolist()\n",
    "    row = wd.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = wd.Weekday.astype('category', categories=Weekday_u).cat.codes\n",
    "    Weekday_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(Weekday_u)))\n",
    "\n",
    "    # Create a sparse matrix of number of items by Upc for each VisitNumber\n",
    "    data = df['ScanCount'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.Upc.astype('category', categories=Upc_all).cat.codes\n",
    "    Upc_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(Upc_all)))\n",
    "\n",
    "    # Create a sparse matrix of number of items by FinelineNumber for each VisitNumber\n",
    "    data = df['ItemsFN'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.FinelineNumber.astype('category', categories=FN_all).cat.codes\n",
    "    FN_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(FN_all)))\n",
    "    \n",
    "    # Create a sparse matrix of number of items by DepartmentDescription for each VisitNumber\n",
    "    data = df['ItemsDD'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.DepartmentDescription.astype('category', categories=DD_all).cat.codes\n",
    "    DD_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(DD_all)))\n",
    "\n",
    "    # Create a sparse matrix of the high level aggregate features\n",
    "    aggs_u = ['NumItems', 'NumProducts', 'Return']\n",
    "    aggs_sm = csr_matrix(aggs.values)\n",
    "    aggs_sm\n",
    "\n",
    "    # Horizontally stack the blocks\n",
    "    sm_m = hstack(blocks=[aggs_sm,Weekday_sm,Upc_sm,FN_sm,DD_sm],format='csr')\n",
    "    sm_l = hstack(blocks=[aggs_sm,Weekday_sm,DD_sm],format='csr')\n",
    "    print sm_m.shape\n",
    "    print sm_l.shape\n",
    "\n",
    "    return sm_l, VisitNumber_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(df):\n",
    "    # Aggregate number of items by VisitNumber, i.e sum ScanCount\n",
    "    tt = df.groupby(['VisitNumber','TripType'], as_index=False)['ScanCount'].sum()\n",
    "    tt = tt.rename(columns={'ScanCount': 'NumItems'})\n",
    "    target = tt.TripType\n",
    "    return target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_run(X, y, single_classifier=True):\n",
    "    # Make a log-loss scorer for use in GridSearchCV\n",
    "    my_log_loss = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)    # Fit model using a single classifier on training data sparse matrix (sm)\n",
    "    if single_classifier:\n",
    "        # Set up classifier\n",
    "        clf = LogisticRegression(C=1000,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=1,max_iter=400,warm_start=True)\n",
    "        # Fit grid search\n",
    "        clf.fit(X,y)\n",
    "        return clf\n",
    "    # Fit model using GridSearchCV on the training data sparse matrix (sm)\n",
    "    else:\n",
    "        # Set up classifier\n",
    "        clf = MultinomialNB()\n",
    "        # clf = LogisticRegression(C=1000,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=1,max_iter=200,warm_start=True)\n",
    "        # clf = LogisticRegression(C=10,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=0.0001,max_iter=200,warm_start=True)\n",
    "        # Set up grid search\n",
    "        # gs = GridSearchCV(estimator=clf, param_grid={'alpha': [round(float(i)/100,2) for i in range(20,31)]},n_jobs=-1,cv=4)\n",
    "        gs = GridSearchCV(estimator=clf, param_grid={'alpha': [round(float(i)/100,2) for i in range(20,31)]},n_jobs=-1,cv=4,scoring=my_log_loss)\n",
    "        # Fit the grid search\n",
    "        gs.fit(X,y) \n",
    "        return gs\n",
    " \n",
    "def decision_tree(X,y):\n",
    "    dt = DecisionTreeClassifier(max_depth=20,max_leaf_nodes=40)\n",
    "    dt.fit(X,y)\n",
    "    return dt\n",
    "\n",
    "def random_forest(X,y):\n",
    "    rf = RandomForestClassifier(max_depth=20,max_leaf_nodes=40)\n",
    "    rf.fit(X,y)\n",
    "    return rf\n",
    "\n",
    "def gradient_boosting(X,y):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X,y)\n",
    "    return gb\n",
    "\n",
    "def print_gridsearch(gs):\n",
    "    # Report grid search results \n",
    "    print gs.grid_scores_\n",
    "    print gs.best_estimator_\n",
    "    print gs.best_score_\n",
    "    print gs.best_params_\n",
    "    \n",
    "def predictions(clf, X, y, ys):\n",
    "    # Make predictions on the training data sparse matrix (sm)\n",
    "    train_preds = clf.predict(X) \n",
    "\n",
    "    # Collect predicted probabilities\n",
    "    train_probs = clf.predict_proba(X)\n",
    "\n",
    "    # Report various accuracy metrics\n",
    "    print \"Log loss:\", round(metrics.log_loss(y,train_probs,eps=1e-15),3)\n",
    "    print \"F1 score:\", round(metrics.f1_score(y,train_preds,average='micro'),3)\n",
    "    print \"\"\n",
    "#     print \"Clasification Report:\"\n",
    "#     print classification_report(y,train_preds)\n",
    "#     print \"\"\n",
    "#     print \"Summary confusion matrix:\"\n",
    "#     cm = confusion_matrix(y,train_preds)\n",
    "#     for i in range(38):\n",
    "#         max_wrong = 0\n",
    "#         k = -1\n",
    "#         for j in range(38):\n",
    "#             if i != j:\n",
    "#                 if cm[i][j] > max_wrong:\n",
    "#                     k = j\n",
    "#                     max_wrong = cm[i][j]\n",
    "#         print ys[i], ys[k], max_wrong     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to write probabilities to a csv file in the correct submission format\n",
    "def write_probs_to_file(vn,tt,probs):\n",
    "    # open file to write results\n",
    "    with open(\"walmart.csv\", \"w\") as results:\n",
    "        # write header\n",
    "        my_str = \"\"\n",
    "        my_str += '\"VisitNumber\"' + \",\" \n",
    "        for trip_num in tt[:-1]:\n",
    "            my_str += '\"TripType_' + str(trip_num) + '\"' + \",\"\n",
    "        my_str += '\"TripType_' + str(tt[-1]) + '\"' + \"\\n\" \n",
    "        results.write(my_str)\n",
    "        # write probs for each visit\n",
    "        for i in range(probs.shape[0]):\n",
    "            my_str = \"\"\n",
    "            my_str += str(vn[i]) + \",\" \n",
    "            for j in range(probs.shape[1]):\n",
    "                my_str += str(probs[i][j]) + \",\"\n",
    "            my_str = my_str[:-1] + \"\\n\"\n",
    "            results.write(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 130127)\n",
      "(95674, 79)\n",
      "(95674, 130127)\n",
      "(95674, 79)\n",
      "(95674, 8)\n",
      "[ 0.07708088  0.18342543  0.15828466  0.12756691  0.1281681   0.12531839\n",
      "  0.12260353  0.06730514]\n",
      "0.989753035803\n"
     ]
    }
   ],
   "source": [
    "# create data sets\n",
    "\n",
    "train_data = get_data('train.csv')\n",
    "del train_data['index']\n",
    "test_data = get_data('test.csv')\n",
    "del test_data['index']\n",
    "target = get_target(train_data)\n",
    "del train_data['TripType']\n",
    "\n",
    "# create sparse matrices\n",
    "train_X, train_visit_numbers = feature_transformation(train_data,test_data,True)\n",
    "train_y = target\n",
    "test_X, test_visit_numbers = feature_transformation(test_data,train_data,True)\n",
    "test_trip_types = sorted(list(set(target)))\n",
    "\n",
    "# scale features to be in the range [-1, 1] based on maximum absolute value of each feature\n",
    "# retains integrity of zero values - good for sparse data\n",
    "scaler = MaxAbsScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "#TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "# reduce the dimensionality of the data\n",
    "dim_reducer = TruncatedSVD(n_components=8)\n",
    "train_X_scaled_dimr = dim_reducer.fit_transform(train_X_scaled)\n",
    "test_X_scaled_dimr = dim_reducer.transform(test_X_scaled)\n",
    "\n",
    "print test_X_scaled_dimr.shape\n",
    "print dim_reducer.explained_variance_ratio_\n",
    "print np.sum(dim_reducer.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.21\n",
      "F1 score: 0.644\n",
      "\n",
      "Log loss: 0.868\n",
      "F1 score: 0.719\n",
      "\n",
      "Log loss: 0.691\n",
      "F1 score: 0.783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting unscaled features\n",
    "for i in [1,3,5]:\n",
    "    clf = GradientBoostingClassifier(max_depth=i)\n",
    "    clf.fit(train_X.toarray(),train_y)\n",
    "    #print_gridsearch(clf)\n",
    "    predictions(clf,train_X.toarray(),train_y,test_trip_types)\n",
    "    test_probs = clf.predict_proba(test_X.toarray())\n",
    "    write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.449\n",
      "F1 score: 0.867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting unscaled features\n",
    "for i in [7]:\n",
    "    clf = GradientBoostingClassifier(max_depth=i)\n",
    "    clf.fit(train_X.toarray(),train_y)\n",
    "    #print_gridsearch(clf)\n",
    "    predictions(clf,train_X.toarray(),train_y,test_trip_types)\n",
    "    test_probs = clf.predict_proba(test_X.toarray())\n",
    "    write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
