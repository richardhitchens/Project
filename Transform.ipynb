{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "from scipy.sparse import *\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "import pygraphviz\n",
    "import graphviz\n",
    "\n",
    "convert_zeroes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data from the file and load into a DataFrame\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # sort by VisitNumber to ensure everything is in the right order  \n",
    "    df = df.sort_values('VisitNumber')\n",
    "    df = df.reset_index()\n",
    "    # del df['index']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to transform data to create new features nd \n",
    "def feature_transformation(df, df_other, convert_zeroes=False):\n",
    "\n",
    "    # Replace nulls will 'Unknown' product categories\n",
    "    df['Upc'] = df['Upc'].fillna('UnknownUpc')\n",
    "    df['DepartmentDescription'] = df['DepartmentDescription'].fillna('UnknownDD')\n",
    "    df['FinelineNumber'] = df['FinelineNumber'].fillna('UnknownFN')\n",
    "\n",
    "    # Replace nulls will 'Unknown' product categories\n",
    "    df_other['Upc'] = df_other['Upc'].fillna('UnknownUpc')\n",
    "    df_other['DepartmentDescription'] = df_other['DepartmentDescription'].fillna('UnknownDD')\n",
    "    df_other['FinelineNumber'] = df_other['FinelineNumber'].fillna('UnknownFN')\n",
    "    \n",
    "    # Create a group of field headers that include categories from both the training and the test data sets\n",
    "    VisitNumber_u = list(sorted(df.VisitNumber.unique()))\n",
    "    VisitNumber_u_other = list(sorted(df_other.VisitNumber.unique()))\n",
    "\n",
    "    Upc_u = list(sorted(df.Upc.unique()))\n",
    "    Upc_u_other = list(sorted(df_other.Upc.unique()))\n",
    "    Upc_all = sorted(list(set(list(set(Upc_u)|set(Upc_u_other)))))\n",
    "\n",
    "    FN_u = list(sorted(df.FinelineNumber.unique()))\n",
    "    FN_u_other = list(sorted(df_other.FinelineNumber.unique()))\n",
    "    FN_all = sorted(list(set(FN_u)|set(FN_u_other)))\n",
    "\n",
    "    DD_u = list(sorted(df.DepartmentDescription.unique()))\n",
    "    DD_u_other = list(sorted(df_other.DepartmentDescription.unique()))\n",
    "    DD_all = sorted(list(set(DD_u)|set(DD_u_other)))\n",
    "    \n",
    "    # Convert Weekday text variable to numerical\n",
    "    df['Weekday'] = df['Weekday'].map({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
    " \n",
    "    # Create a return dummy\n",
    "    df['Return'] = np.where(df['ScanCount'] > 0, 0 , 1)\n",
    "\n",
    "    # Convert ScanCount negatives to 0's\n",
    "    if convert_zeroes:\n",
    "        df['ScanCount'] = np.where(df['ScanCount'] < 0 , 0, df['ScanCount'])\n",
    "\n",
    "    # Aggregate number of items by Department Description, i.e. sum ScanCount\n",
    "    dd = df.groupby(['VisitNumber','DepartmentDescription'], as_index=False)['ScanCount'].sum()\n",
    "    dd = dd.rename(columns={'ScanCount': 'ItemsDD'})\n",
    "    df = pd.merge(left=df, right=dd, on=['VisitNumber','DepartmentDescription'], how='left')\n",
    "\n",
    "    # Aggregate number of items by FinelineNumber, i.e. sum ScanCount\n",
    "    fn = df.groupby(['VisitNumber','FinelineNumber'], as_index=False)['ScanCount'].sum()\n",
    "    fn = fn.rename(columns={'ScanCount': 'ItemsFN'})\n",
    "    df = pd.merge(left=df, right=fn, on=['VisitNumber','FinelineNumber'], how='left')        \n",
    "\n",
    "    # Aggregate number of products by VisitNumber, i.e. count ScanCount\n",
    "    wd = df.groupby(['VisitNumber','Weekday'], as_index=False)['ScanCount'].count()\n",
    "    wd = wd.rename(columns={'ScanCount': 'NumProducts'})\n",
    "    Weekday_u = list(sorted(wd.Weekday.unique()))\n",
    "    df = pd.merge(left=df, right=wd, on=['VisitNumber','Weekday'], how='left')\n",
    "    \n",
    "    # Create a return dummy for each shopping visit\n",
    "    rt = df.groupby(['VisitNumber'], as_index=False)['Return'].sum()\n",
    "    rt['Return'] = np.where(rt['Return'] > 0 , 1, 0)\n",
    "\n",
    "    # Aggregate number of items by VisitNumber, i.e sum ScanCount\n",
    "    tt = df.groupby(['VisitNumber'], as_index=False)['ScanCount'].sum()\n",
    "    tt = tt.rename(columns={'ScanCount': 'NumItems'})\n",
    "    df = pd.merge(left=df, right=tt, on=['VisitNumber'], how='left')\n",
    "    \n",
    "    # Combine aggregates and sort by VisitNumber to get ordered row names of VisitNumber\n",
    "    tt['NumProducts'] = wd.NumProducts\n",
    "    tt['Return'] = rt.Return\n",
    "    tt.sort_values('VisitNumber')\n",
    "    aggs = tt[['NumItems', 'NumProducts', 'Return']] \n",
    "\n",
    "    # Isolate Visit Numbers\n",
    "    visit_numbers = tt.VisitNumber\n",
    "\n",
    "    # Create a sparse matrix of Weekday dummies for VisitNumber\n",
    "    data = wd['Weekday'].tolist()\n",
    "    row = wd.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = wd.Weekday.astype('category', categories=Weekday_u).cat.codes\n",
    "    Weekday_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(Weekday_u)))\n",
    "\n",
    "    # Create a sparse matrix of number of items by Upc for each VisitNumber\n",
    "    data = df['ScanCount'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.Upc.astype('category', categories=Upc_all).cat.codes\n",
    "    Upc_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(Upc_all)))\n",
    "\n",
    "    # Create a sparse matrix of number of items by FinelineNumber for each VisitNumber\n",
    "    data = df['ItemsFN'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.FinelineNumber.astype('category', categories=FN_all).cat.codes\n",
    "    FN_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(FN_all)))\n",
    "    \n",
    "    # Create a sparse matrix of number of items by DepartmentDescription for each VisitNumber\n",
    "    data = df['ItemsDD'].tolist()\n",
    "    row = df.VisitNumber.astype('category', categories=VisitNumber_u).cat.codes\n",
    "    col = df.DepartmentDescription.astype('category', categories=DD_all).cat.codes\n",
    "    DD_sm = csr_matrix((data, (row, col)), shape=(len(VisitNumber_u), len(DD_all)))\n",
    "\n",
    "    # Create a sparse matrix of the high level aggregate features\n",
    "    aggs_u = ['NumItems', 'NumProducts', 'Return']\n",
    "    aggs_sm = csr_matrix(aggs.values)\n",
    "    aggs_sm\n",
    "\n",
    "    # Horizontally stack the blocks\n",
    "    sm_m = hstack(blocks=[aggs_sm,Weekday_sm,Upc_sm,FN_sm,DD_sm],format='csr')\n",
    "    sm_l = hstack(blocks=[aggs_sm,Weekday_sm,DD_sm],format='csr')\n",
    "    print sm_m.shape\n",
    "    print sm_l.shape\n",
    "\n",
    "    return sm_l, VisitNumber_u\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(df):\n",
    "    # Aggregate number of items by VisitNumber, i.e sum ScanCount\n",
    "    tt = df.groupby(['VisitNumber','TripType'], as_index=False)['ScanCount'].sum()\n",
    "    tt = tt.rename(columns={'ScanCount': 'NumItems'})\n",
    "    target = tt.TripType\n",
    "    return target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_run(X, y, single_classifier=True):\n",
    "    # Make a log-loss scorer for use in GridSearchCV\n",
    "    my_log_loss = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)    # Fit model using a single classifier on training data sparse matrix (sm)\n",
    "    if single_classifier:\n",
    "        # Set up classifier\n",
    "        clf = LogisticRegression(C=1000,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=1,max_iter=400,warm_start=True)\n",
    "        # Fit grid search\n",
    "        clf.fit(X,y)\n",
    "        return clf\n",
    "    # Fit model using GridSearchCV on the training data sparse matrix (sm)\n",
    "    else:\n",
    "        # Set up classifier\n",
    "        clf = MultinomialNB()\n",
    "        # clf = LogisticRegression(C=1000,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=1,max_iter=200,warm_start=True)\n",
    "        # clf = LogisticRegression(C=10,multi_class='multinomial',solver='newton-cg',n_jobs=-1,tol=0.0001,max_iter=200,warm_start=True)\n",
    "        # Set up grid search\n",
    "        # gs = GridSearchCV(estimator=clf, param_grid={'alpha': [round(float(i)/100,2) for i in range(20,31)]},n_jobs=-1,cv=4)\n",
    "        gs = GridSearchCV(estimator=clf, param_grid={'alpha': [round(float(i)/100,2) for i in range(20,31)]},n_jobs=-1,cv=4,scoring=my_log_loss)\n",
    "        # Fit the grid search\n",
    "        gs.fit(X,y) \n",
    "        return gs\n",
    " \n",
    "def decision_tree(X,y):\n",
    "    dt = DecisionTreeClassifier(max_depth=20,max_leaf_nodes=40)\n",
    "    dt.fit(X,y)\n",
    "    return dt\n",
    "\n",
    "def random_forest(X,y):\n",
    "    rf = RandomForestClassifier(max_depth=20,max_leaf_nodes=40)\n",
    "    rf.fit(X,y)\n",
    "    return rf\n",
    "\n",
    "def gradient_boosting(X,y):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X,y)\n",
    "    return gb\n",
    "\n",
    "def print_gridsearch(gs):\n",
    "    # Report grid search results \n",
    "    print gs.grid_scores_\n",
    "    print gs.best_estimator_\n",
    "    print gs.best_score_\n",
    "    print gs.best_params_\n",
    "    \n",
    "def predictions(clf, X, y, ys):\n",
    "    # Make predictions on the training data sparse matrix (sm)\n",
    "    train_preds = clf.predict(X) \n",
    "\n",
    "    # Collect predicted probabilities\n",
    "    train_probs = clf.predict_proba(X)\n",
    "\n",
    "    # Report various accuracy metrics\n",
    "    print \"Log loss:\", round(metrics.log_loss(y,train_probs,eps=1e-15),3)\n",
    "    print \"F1 score:\", round(metrics.f1_score(y,train_preds,average='micro'),3)\n",
    "    print \"\"\n",
    "    print \"Clasification Report:\"\n",
    "    print classification_report(y,train_preds)\n",
    "    print \"\"\n",
    "    print \"Summary confusion matrix:\"\n",
    "    cm = confusion_matrix(y,train_preds)\n",
    "    for i in range(38):\n",
    "        max_wrong = 0\n",
    "        k = -1\n",
    "        for j in range(38):\n",
    "            if i != j:\n",
    "                if cm[i][j] > max_wrong:\n",
    "                    k = j\n",
    "                    max_wrong = cm[i][j]\n",
    "        print ys[i], ys[k], max_wrong     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to write probabilities to a csv file in the correct submission format\n",
    "def write_probs_to_file(vn,tt,probs):\n",
    "    # open file to write results\n",
    "    with open(\"walmart.csv\", \"w\") as results:\n",
    "        # write header\n",
    "        my_str = \"\"\n",
    "        my_str += '\"VisitNumber\"' + \",\" \n",
    "        for trip_num in tt[:-1]:\n",
    "            my_str += '\"TripType_' + str(trip_num) + '\"' + \",\"\n",
    "        my_str += '\"TripType_' + str(tt[-1]) + '\"' + \"\\n\" \n",
    "        results.write(my_str)\n",
    "        # write probs for each visit\n",
    "        for i in range(probs.shape[0]):\n",
    "            my_str = \"\"\n",
    "            my_str += str(vn[i]) + \",\" \n",
    "            for j in range(probs.shape[1]):\n",
    "                my_str += str(probs[i][j]) + \",\"\n",
    "            my_str = my_str[:-1] + \"\\n\"\n",
    "            results.write(my_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 130127)\n",
      "(95674, 79)\n",
      "(95674, 130127)\n",
      "(95674, 79)\n",
      "(95674, 8)\n",
      "[ 0.07708088  0.18342543  0.15828466  0.12756691  0.1281681   0.12531839\n",
      "  0.12260353  0.06730514]\n",
      "0.989753035803\n"
     ]
    }
   ],
   "source": [
    "# create data sets\n",
    "\n",
    "train_data = get_data('train.csv')\n",
    "del train_data['index']\n",
    "test_data = get_data('test.csv')\n",
    "del test_data['index']\n",
    "target = get_target(train_data)\n",
    "del train_data['TripType']\n",
    "\n",
    "# create sparse matrices\n",
    "train_X, train_visit_numbers = feature_transformation(train_data,test_data,True)\n",
    "train_y = target\n",
    "test_X, test_visit_numbers = feature_transformation(test_data,train_data,True)\n",
    "test_trip_types = sorted(list(set(target)))\n",
    "\n",
    "# scale features to be in the range [-1, 1] based on maximum absolute value of each feature\n",
    "# retains integrity of zero values - good for sparse data\n",
    "scaler = MaxAbsScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "#TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "# reduce the dimensionality of the data\n",
    "dim_reducer = TruncatedSVD(n_components=8)\n",
    "train_X_scaled_dimr = dim_reducer.fit_transform(train_X_scaled)\n",
    "test_X_scaled_dimr = dim_reducer.transform(test_X_scaled)\n",
    "\n",
    "print test_X_scaled_dimr.shape\n",
    "print dim_reducer.explained_variance_ratio_\n",
    "print np.sum(dim_reducer.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_X_scaled_dimr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.886\n",
      "F1 score: 0.719\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.77      0.99      0.87      3643\n",
      "          4       0.17      0.04      0.07       346\n",
      "          5       0.74      0.85      0.79      4593\n",
      "          6       0.72      0.75      0.74      1277\n",
      "          7       0.70      0.70      0.70      5752\n",
      "          8       0.79      0.85      0.82     12161\n",
      "          9       0.72      0.76      0.74      9464\n",
      "         12       0.75      0.25      0.37       269\n",
      "         14       1.00      1.00      1.00         4\n",
      "         15       0.71      0.47      0.56       978\n",
      "         18       0.50      0.50      0.50       549\n",
      "         19       0.57      0.37      0.45       375\n",
      "         20       0.67      0.66      0.67       637\n",
      "         21       0.69      0.75      0.72       641\n",
      "         22       0.52      0.47      0.50       928\n",
      "         23       0.54      0.63      0.58       139\n",
      "         24       0.64      0.66      0.65      2609\n",
      "         25       0.74      0.79      0.76      3698\n",
      "         26       0.60      0.55      0.57       503\n",
      "         27       0.68      0.71      0.69       785\n",
      "         28       0.50      0.52      0.51       492\n",
      "         29       0.68      0.16      0.26       433\n",
      "         30       0.54      0.47      0.50      1081\n",
      "         31       0.68      0.86      0.76       594\n",
      "         32       0.67      0.80      0.73      1984\n",
      "         33       0.66      0.64      0.65      1315\n",
      "         34       0.65      0.67      0.66       719\n",
      "         35       0.63      0.63      0.63      2030\n",
      "         36       0.66      0.68      0.67      3005\n",
      "         37       0.68      0.61      0.64      2788\n",
      "         38       0.60      0.44      0.51      2912\n",
      "         39       0.59      0.71      0.64      9896\n",
      "         40       0.82      0.93      0.87      6130\n",
      "         41       0.73      0.28      0.40       583\n",
      "         42       0.69      0.36      0.48      1858\n",
      "         43       0.63      0.11      0.19       872\n",
      "         44       0.72      0.27      0.40      1187\n",
      "        999       0.95      0.75      0.84      8444\n",
      "\n",
      "avg / total       0.72      0.72      0.71     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 22 7\n",
      "4 5 294\n",
      "5 39 231\n",
      "6 8 146\n",
      "7 8 661\n",
      "8 7 431\n",
      "9 8 788\n",
      "12 39 46\n",
      "14 999 0\n",
      "15 39 113\n",
      "18 9 98\n",
      "19 9 146\n",
      "20 9 126\n",
      "21 9 41\n",
      "22 9 266\n",
      "23 9 30\n",
      "24 9 247\n",
      "25 9 251\n",
      "26 9 96\n",
      "27 9 94\n",
      "28 9 123\n",
      "29 9 124\n",
      "30 9 280\n",
      "31 9 32\n",
      "32 39 116\n",
      "33 39 249\n",
      "34 39 81\n",
      "35 39 335\n",
      "36 39 418\n",
      "37 39 323\n",
      "38 39 835\n",
      "39 7 445\n",
      "40 39 211\n",
      "41 25 70\n",
      "42 25 235\n",
      "43 39 221\n",
      "44 39 338\n",
      "999 3 716\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting unscaled features\n",
    "clf = gradient_boosting(train_X.toarray(),train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X.toarray(),train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X.toarray())\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # XGBoost\n",
    "\n",
    "# #transform sparse matrices to XGBoost DMatrix\n",
    "# xgb_train = xgb.DMatrix(train_X)\n",
    "# xgb_test = xgb.DMatrix(test_X)\n",
    "\n",
    "# # save to binary files for faster loading \n",
    "# xgb_train.save_binary(\"train.buffer\")\n",
    "# xgb_test.save_binary(\"test.buffer\")\n",
    "\n",
    "# # specify validations set to watch performance\n",
    "# evallist  = [(xgb_test,'eval'), (xgb_train,'train')]\n",
    "\n",
    "# # In setting parameters we have selected:\n",
    "\n",
    "# # 'objective':'multi:softprob': It output a vector of ndata * nclass, which can be further reshaped to ndata, nclass matrix.\n",
    "# # The result contains predicted probability of each data point belonging to each class.\n",
    "# # 'eval_metric':'mlogloss': Multi-class log-loss\n",
    "\n",
    "# param = {'max_depth':2, 'eta':1, 'silent':0, 'objective':'multi:softprob', 'eval_metric':'mlogloss'}\n",
    "\n",
    "# num_round = 1\n",
    "# #xgb.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, early_stopping_rounds=None, evals_result=None)\n",
    "# # bst = xgb.train(params=param, dtrain=xgb_train, num_boost_round=num_round, evals=evallist, early_stopping_rounds=10)\n",
    "\n",
    "# # ypred = bst.predict(xgb_test,ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "# # print ypred.shape\n",
    "\n",
    "# # xgb.to_graphviz(bst, num_trees=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -5.73986, std: 0.13753, params: {'alpha': 0.2}, mean: -5.74007, std: 0.13758, params: {'alpha': 0.21}, mean: -5.74028, std: 0.13763, params: {'alpha': 0.22}, mean: -5.74047, std: 0.13766, params: {'alpha': 0.23}, mean: -5.74065, std: 0.13768, params: {'alpha': 0.24}, mean: -5.74082, std: 0.13768, params: {'alpha': 0.25}, mean: -5.74098, std: 0.13767, params: {'alpha': 0.26}, mean: -5.74113, std: 0.13765, params: {'alpha': 0.27}, mean: -5.74127, std: 0.13764, params: {'alpha': 0.28}, mean: -5.74141, std: 0.13761, params: {'alpha': 0.29}, mean: -5.74154, std: 0.13758, params: {'alpha': 0.3}]\n",
      "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)\n",
      "-5.73985763736\n",
      "{'alpha': 0.2}\n",
      "Log loss: 5.64\n",
      "F1 score: 0.541\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.79      0.85      0.82      3643\n",
      "          4       0.08      0.19      0.12       346\n",
      "          5       0.70      0.60      0.65      4593\n",
      "          6       0.43      0.55      0.48      1277\n",
      "          7       0.50      0.58      0.54      5752\n",
      "          8       0.58      0.70      0.63     12161\n",
      "          9       0.60      0.68      0.64      9464\n",
      "         12       0.09      0.32      0.14       269\n",
      "         14       0.02      0.50      0.03         4\n",
      "         15       0.42      0.43      0.43       978\n",
      "         18       0.33      0.50      0.40       549\n",
      "         19       0.29      0.31      0.30       375\n",
      "         20       0.50      0.70      0.58       637\n",
      "         21       0.43      0.58      0.49       641\n",
      "         22       0.42      0.23      0.30       928\n",
      "         23       0.22      0.35      0.27       139\n",
      "         24       0.57      0.48      0.52      2609\n",
      "         25       0.68      0.64      0.66      3698\n",
      "         26       0.36      0.54      0.43       503\n",
      "         27       0.56      0.66      0.61       785\n",
      "         28       0.43      0.47      0.45       492\n",
      "         29       0.18      0.09      0.12       433\n",
      "         30       0.36      0.38      0.37      1081\n",
      "         31       0.69      0.59      0.64       594\n",
      "         32       0.68      0.63      0.65      1984\n",
      "         33       0.42      0.63      0.50      1315\n",
      "         34       0.44      0.65      0.52       719\n",
      "         35       0.35      0.63      0.45      2030\n",
      "         36       0.47      0.65      0.54      3005\n",
      "         37       0.48      0.62      0.54      2788\n",
      "         38       0.35      0.52      0.42      2912\n",
      "         39       0.48      0.23      0.31      9896\n",
      "         40       0.65      0.31      0.42      6130\n",
      "         41       0.13      0.07      0.09       583\n",
      "         42       0.45      0.09      0.15      1858\n",
      "         43       0.13      0.18      0.15       872\n",
      "         44       0.32      0.13      0.18      1187\n",
      "        999       0.81      0.67      0.73      8444\n",
      "\n",
      "avg / total       0.56      0.54      0.53     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 999 303\n",
      "4 5 158\n",
      "5 8 626\n",
      "6 8 418\n",
      "7 8 1288\n",
      "8 7 698\n",
      "9 8 501\n",
      "12 27 30\n",
      "14 15 1\n",
      "15 8 107\n",
      "18 9 91\n",
      "19 9 164\n",
      "20 9 81\n",
      "21 14 73\n",
      "22 9 342\n",
      "23 9 63\n",
      "24 9 489\n",
      "25 9 476\n",
      "26 9 84\n",
      "27 9 103\n",
      "28 9 114\n",
      "29 18 108\n",
      "30 9 328\n",
      "31 9 165\n",
      "32 8 134\n",
      "33 8 119\n",
      "34 8 92\n",
      "35 8 336\n",
      "36 8 353\n",
      "37 7 370\n",
      "38 7 319\n",
      "39 7 1045\n",
      "40 39 886\n",
      "41 9 101\n",
      "42 9 234\n",
      "43 24 58\n",
      "44 43 122\n",
      "999 3 710\n"
     ]
    }
   ],
   "source": [
    "# Multinomial unscaled features\n",
    "clf = model_run(train_X,train_y,False)\n",
    "print_gridsearch(clf)\n",
    "predictions(clf,train_X,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -2.82433, std: 0.00264, params: {'alpha': 0.2}, mean: -2.82482, std: 0.00265, params: {'alpha': 0.21}, mean: -2.82530, std: 0.00266, params: {'alpha': 0.22}, mean: -2.82576, std: 0.00267, params: {'alpha': 0.23}, mean: -2.82622, std: 0.00268, params: {'alpha': 0.24}, mean: -2.82666, std: 0.00268, params: {'alpha': 0.25}, mean: -2.82709, std: 0.00269, params: {'alpha': 0.26}, mean: -2.82751, std: 0.00270, params: {'alpha': 0.27}, mean: -2.82791, std: 0.00271, params: {'alpha': 0.28}, mean: -2.82831, std: 0.00272, params: {'alpha': 0.29}, mean: -2.82871, std: 0.00272, params: {'alpha': 0.3}]\n",
      "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)\n",
      "-2.82433119818\n",
      "{'alpha': 0.2}\n",
      "Log loss: 2.809\n",
      "F1 score: 0.232\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.99      0.03      0.05      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.97      0.03      0.06      4593\n",
      "          6       1.00      0.00      0.00      1277\n",
      "          7       0.33      0.00      0.00      5752\n",
      "          8       0.17      0.98      0.29     12161\n",
      "          9       0.08      0.01      0.02      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.88      0.01      0.01       978\n",
      "         18       0.80      0.01      0.01       549\n",
      "         19       1.00      0.00      0.01       375\n",
      "         20       0.00      0.00      0.00       637\n",
      "         21       0.50      0.00      0.01       641\n",
      "         22       0.75      0.01      0.01       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.89      0.01      0.02      2609\n",
      "         25       0.89      0.03      0.06      3698\n",
      "         26       0.67      0.01      0.02       503\n",
      "         27       0.79      0.01      0.03       785\n",
      "         28       0.57      0.01      0.02       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.50      0.00      0.01      1081\n",
      "         31       1.00      0.00      0.00       594\n",
      "         32       1.00      0.00      0.00      1984\n",
      "         33       0.00      0.00      0.00      1315\n",
      "         34       0.00      0.00      0.00       719\n",
      "         35       0.00      0.00      0.00      2030\n",
      "         36       1.00      0.00      0.00      3005\n",
      "         37       0.00      0.00      0.00      2788\n",
      "         38       1.00      0.00      0.00      2912\n",
      "         39       0.16      0.20      0.18      9896\n",
      "         40       0.69      0.22      0.33      6130\n",
      "         41       1.00      0.00      0.00       583\n",
      "         42       0.67      0.00      0.01      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.33      0.00      0.00      1187\n",
      "        999       0.62      0.77      0.69      8444\n",
      "\n",
      "avg / total       0.48      0.23      0.15     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 8 3143\n",
      "4 8 332\n",
      "5 8 4243\n",
      "6 8 1171\n",
      "7 8 5041\n",
      "8 999 272\n",
      "9 8 9092\n",
      "12 8 168\n",
      "14 8 2\n",
      "15 8 781\n",
      "18 8 424\n",
      "19 8 286\n",
      "20 8 525\n",
      "21 8 543\n",
      "22 8 684\n",
      "23 8 124\n",
      "24 8 1996\n",
      "25 8 2742\n",
      "26 8 384\n",
      "27 8 513\n",
      "28 8 406\n",
      "29 8 364\n",
      "30 8 930\n",
      "31 8 366\n",
      "32 8 1708\n",
      "33 8 971\n",
      "34 8 611\n",
      "35 8 1791\n",
      "36 8 2612\n",
      "37 8 1462\n",
      "38 8 2009\n",
      "39 8 7297\n",
      "40 39 3918\n",
      "41 8 384\n",
      "42 8 1154\n",
      "43 8 733\n",
      "44 39 562\n",
      "999 8 1886\n"
     ]
    }
   ],
   "source": [
    "# Multinomial scaled features\n",
    "clf = model_run(train_X_scaled,train_y,False)\n",
    "print_gridsearch(clf)\n",
    "predictions(clf,train_X_scaled,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X_scaled)\n",
    "#test_probs = clf.predict(test_X_scaled)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.445\n",
      "F1 score: 0.337\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.00      0.00      0.00      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.00      0.00      0.00      4593\n",
      "          6       0.00      0.00      0.00      1277\n",
      "          7       0.11      0.04      0.06      5752\n",
      "          8       0.26      0.97      0.42     12161\n",
      "          9       0.00      0.00      0.00      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.00      0.00      0.00       978\n",
      "         18       0.00      0.00      0.00       549\n",
      "         19       0.00      0.00      0.00       375\n",
      "         20       0.00      0.00      0.00       637\n",
      "         21       0.00      0.00      0.00       641\n",
      "         22       0.00      0.00      0.00       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.00      0.00      0.00      2609\n",
      "         25       0.00      0.00      0.00      3698\n",
      "         26       0.00      0.00      0.00       503\n",
      "         27       0.00      0.00      0.00       785\n",
      "         28       0.00      0.00      0.00       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.00      0.00      0.00      1081\n",
      "         31       0.00      0.00      0.00       594\n",
      "         32       0.00      0.00      0.00      1984\n",
      "         33       0.00      0.00      0.00      1315\n",
      "         34       0.00      0.00      0.00       719\n",
      "         35       0.00      0.00      0.00      2030\n",
      "         36       0.00      0.00      0.00      3005\n",
      "         37       0.00      0.00      0.00      2788\n",
      "         38       0.00      0.00      0.00      2912\n",
      "         39       0.26      0.89      0.41      9896\n",
      "         40       0.66      0.83      0.74      6130\n",
      "         41       0.00      0.00      0.00       583\n",
      "         42       0.00      0.00      0.00      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.00      0.00      0.00      1187\n",
      "        999       0.79      0.75      0.77      8444\n",
      "\n",
      "avg / total       0.18      0.34      0.21     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 8 3444\n",
      "4 8 277\n",
      "5 8 3488\n",
      "6 8 1019\n",
      "7 8 3340\n",
      "8 999 270\n",
      "9 8 9138\n",
      "12 39 182\n",
      "14 39 2\n",
      "15 39 589\n",
      "18 39 273\n",
      "19 8 260\n",
      "20 8 292\n",
      "21 39 357\n",
      "22 8 590\n",
      "23 8 112\n",
      "24 39 1388\n",
      "25 39 2277\n",
      "26 39 238\n",
      "27 39 443\n",
      "28 8 236\n",
      "29 8 225\n",
      "30 8 588\n",
      "31 8 455\n",
      "32 39 1098\n",
      "33 39 890\n",
      "34 39 428\n",
      "35 39 1226\n",
      "36 39 1924\n",
      "37 39 1654\n",
      "38 39 1879\n",
      "39 8 707\n",
      "40 39 1005\n",
      "41 39 435\n",
      "42 39 1375\n",
      "43 39 750\n",
      "44 39 808\n",
      "999 8 1813\n"
     ]
    }
   ],
   "source": [
    "# Multinomial scaled and TruncatedSVD features\n",
    "clf = model_run(train_X_scaled_dimr,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X_scaled_dimr,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X_scaled_dimr)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.797\n",
      "F1 score: 0.526\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.77      0.88      0.82      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.63      0.75      0.69      4593\n",
      "          6       0.88      0.31      0.46      1277\n",
      "          7       0.77      0.33      0.46      5752\n",
      "          8       0.75      0.57      0.65     12161\n",
      "          9       0.45      0.83      0.58      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.00      0.00      0.00       978\n",
      "         18       0.00      0.00      0.00       549\n",
      "         19       0.00      0.00      0.00       375\n",
      "         20       0.00      0.00      0.00       637\n",
      "         21       0.00      0.00      0.00       641\n",
      "         22       0.00      0.00      0.00       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.00      0.00      0.00      2609\n",
      "         25       0.58      0.50      0.54      3698\n",
      "         26       0.00      0.00      0.00       503\n",
      "         27       0.00      0.00      0.00       785\n",
      "         28       0.00      0.00      0.00       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.00      0.00      0.00      1081\n",
      "         31       0.00      0.00      0.00       594\n",
      "         32       0.71      0.45      0.55      1984\n",
      "         33       0.45      0.40      0.43      1315\n",
      "         34       0.00      0.00      0.00       719\n",
      "         35       0.46      0.32      0.38      2030\n",
      "         36       0.54      0.44      0.49      3005\n",
      "         37       0.44      0.46      0.45      2788\n",
      "         38       0.67      0.13      0.22      2912\n",
      "         39       0.28      0.77      0.41      9896\n",
      "         40       0.64      0.94      0.76      6130\n",
      "         41       0.00      0.00      0.00       583\n",
      "         42       0.00      0.00      0.00      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.00      0.00      0.00      1187\n",
      "        999       0.92      0.74      0.82      8444\n",
      "\n",
      "avg / total       0.51      0.53      0.48     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 9 214\n",
      "4 5 300\n",
      "5 39 583\n",
      "6 9 397\n",
      "7 39 1974\n",
      "8 9 3937\n",
      "9 8 868\n",
      "12 39 188\n",
      "14 39 3\n",
      "15 39 639\n",
      "18 39 338\n",
      "19 9 191\n",
      "20 39 389\n",
      "21 39 520\n",
      "22 9 414\n",
      "23 9 96\n",
      "24 39 1516\n",
      "25 39 1027\n",
      "26 39 314\n",
      "27 39 535\n",
      "28 39 289\n",
      "29 39 206\n",
      "30 39 452\n",
      "31 9 382\n",
      "32 39 534\n",
      "33 39 511\n",
      "34 39 482\n",
      "35 39 1138\n",
      "36 39 1159\n",
      "37 39 646\n",
      "38 39 1809\n",
      "39 36 444\n",
      "40 39 190\n",
      "41 39 347\n",
      "42 39 1090\n",
      "43 39 606\n",
      "44 39 461\n",
      "999 9 726\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree unscaled features\n",
    "clf = decision_tree(train_X,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.709\n",
      "F1 score: 0.549\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.77      0.88      0.82      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.65      0.80      0.71      4593\n",
      "          6       0.63      0.55      0.59      1277\n",
      "          7       0.77      0.31      0.44      5752\n",
      "          8       0.73      0.80      0.76     12161\n",
      "          9       0.59      0.73      0.66      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.00      0.00      0.00       978\n",
      "         18       0.00      0.00      0.00       549\n",
      "         19       0.00      0.00      0.00       375\n",
      "         20       0.62      0.44      0.52       637\n",
      "         21       0.00      0.00      0.00       641\n",
      "         22       0.00      0.00      0.00       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.50      0.18      0.26      2609\n",
      "         25       0.58      0.50      0.54      3698\n",
      "         26       0.00      0.00      0.00       503\n",
      "         27       0.00      0.00      0.00       785\n",
      "         28       0.00      0.00      0.00       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.00      0.00      0.00      1081\n",
      "         31       0.58      0.57      0.58       594\n",
      "         32       0.71      0.45      0.55      1984\n",
      "         33       0.45      0.40      0.43      1315\n",
      "         34       0.59      0.46      0.52       719\n",
      "         35       0.00      0.00      0.00      2030\n",
      "         36       0.37      0.55      0.45      3005\n",
      "         37       0.41      0.37      0.39      2788\n",
      "         38       0.00      0.00      0.00      2912\n",
      "         39       0.27      0.71      0.39      9896\n",
      "         40       0.63      0.96      0.76      6130\n",
      "         41       0.00      0.00      0.00       583\n",
      "         42       0.00      0.00      0.00      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.00      0.00      0.00      1187\n",
      "        999       0.92      0.74      0.82      8444\n",
      "\n",
      "avg / total       0.51      0.55      0.50     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 9 214\n",
      "4 5 300\n",
      "5 39 554\n",
      "6 39 296\n",
      "7 39 2351\n",
      "8 9 797\n",
      "9 8 1482\n",
      "12 39 189\n",
      "14 39 2\n",
      "15 39 651\n",
      "18 39 328\n",
      "19 9 185\n",
      "20 9 128\n",
      "21 39 502\n",
      "22 9 375\n",
      "23 9 94\n",
      "24 39 962\n",
      "25 39 953\n",
      "26 39 288\n",
      "27 39 490\n",
      "28 39 279\n",
      "29 39 189\n",
      "30 39 438\n",
      "31 39 123\n",
      "32 39 515\n",
      "33 39 478\n",
      "34 39 135\n",
      "35 39 1772\n",
      "36 39 830\n",
      "37 40 799\n",
      "38 39 2255\n",
      "39 36 1210\n",
      "40 39 182\n",
      "41 39 281\n",
      "42 39 862\n",
      "43 39 495\n",
      "44 40 457\n",
      "999 3 712\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree scaled features\n",
    "clf = decision_tree(train_X_scaled,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X_scaled,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X_scaled)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 2.324\n",
      "F1 score: 0.358\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.69      0.52      0.59      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.00      0.00      0.00      4593\n",
      "          6       0.00      0.00      0.00      1277\n",
      "          7       0.11      0.02      0.04      5752\n",
      "          8       0.33      0.95      0.49     12161\n",
      "          9       0.42      0.01      0.01      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.00      0.00      0.00       978\n",
      "         18       0.00      0.00      0.00       549\n",
      "         19       0.00      0.00      0.00       375\n",
      "         20       0.00      0.00      0.00       637\n",
      "         21       0.00      0.00      0.00       641\n",
      "         22       0.00      0.00      0.00       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.00      0.00      0.00      2609\n",
      "         25       0.00      0.00      0.00      3698\n",
      "         26       0.00      0.00      0.00       503\n",
      "         27       0.00      0.00      0.00       785\n",
      "         28       0.00      0.00      0.00       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.00      0.00      0.00      1081\n",
      "         31       0.00      0.00      0.00       594\n",
      "         32       0.00      0.00      0.00      1984\n",
      "         33       0.00      0.00      0.00      1315\n",
      "         34       0.00      0.00      0.00       719\n",
      "         35       0.00      0.00      0.00      2030\n",
      "         36       0.00      0.00      0.00      3005\n",
      "         37       0.00      0.00      0.00      2788\n",
      "         38       0.00      0.00      0.00      2912\n",
      "         39       0.23      0.93      0.37      9896\n",
      "         40       0.63      0.87      0.73      6130\n",
      "         41       0.00      0.00      0.00       583\n",
      "         42       0.00      0.00      0.00      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.00      0.00      0.00      1187\n",
      "        999       0.81      0.72      0.76      8444\n",
      "\n",
      "avg / total       0.25      0.36      0.24     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 8 1398\n",
      "4 8 252\n",
      "5 8 2939\n",
      "6 8 910\n",
      "7 39 2745\n",
      "8 39 310\n",
      "9 8 8782\n",
      "12 39 225\n",
      "14 39 2\n",
      "15 39 744\n",
      "18 39 361\n",
      "19 8 216\n",
      "20 39 411\n",
      "21 39 506\n",
      "22 8 466\n",
      "23 8 104\n",
      "24 39 1723\n",
      "25 39 2897\n",
      "26 39 334\n",
      "27 39 566\n",
      "28 39 304\n",
      "29 39 236\n",
      "30 39 611\n",
      "31 8 357\n",
      "32 39 1317\n",
      "33 39 1117\n",
      "34 39 536\n",
      "35 39 1715\n",
      "36 39 2599\n",
      "37 39 1822\n",
      "38 39 2277\n",
      "39 40 230\n",
      "40 39 763\n",
      "41 39 501\n",
      "42 39 1525\n",
      "43 39 832\n",
      "44 39 744\n",
      "999 8 1236\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree scaled and TruncatedSVD features\n",
    "clf = decision_tree(train_X_scaled_dimr,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X_scaled_dimr,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X_scaled_dimr)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.639\n",
      "F1 score: 0.559\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.69      0.92      0.79      3643\n",
      "          4       0.00      0.00      0.00       346\n",
      "          5       0.74      0.73      0.73      4593\n",
      "          6       0.81      0.33      0.47      1277\n",
      "          7       0.69      0.38      0.49      5752\n",
      "          8       0.62      0.81      0.70     12161\n",
      "          9       0.55      0.78      0.65      9464\n",
      "         12       0.00      0.00      0.00       269\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       0.61      0.16      0.25       978\n",
      "         18       0.00      0.00      0.00       549\n",
      "         19       0.00      0.00      0.00       375\n",
      "         20       0.73      0.28      0.40       637\n",
      "         21       1.00      0.00      0.01       641\n",
      "         22       0.00      0.00      0.00       928\n",
      "         23       0.00      0.00      0.00       139\n",
      "         24       0.60      0.26      0.36      2609\n",
      "         25       0.63      0.52      0.57      3698\n",
      "         26       0.00      0.00      0.00       503\n",
      "         27       0.86      0.08      0.14       785\n",
      "         28       0.00      0.00      0.00       492\n",
      "         29       0.00      0.00      0.00       433\n",
      "         30       0.00      0.00      0.00      1081\n",
      "         31       0.59      0.64      0.61       594\n",
      "         32       0.69      0.55      0.61      1984\n",
      "         33       0.66      0.13      0.22      1315\n",
      "         34       0.67      0.27      0.38       719\n",
      "         35       0.00      0.00      0.00      2030\n",
      "         36       0.50      0.41      0.45      3005\n",
      "         37       0.55      0.11      0.19      2788\n",
      "         38       0.00      0.00      0.00      2912\n",
      "         39       0.32      0.88      0.47      9896\n",
      "         40       0.63      0.93      0.75      6130\n",
      "         41       0.00      0.00      0.00       583\n",
      "         42       0.00      0.00      0.00      1858\n",
      "         43       0.00      0.00      0.00       872\n",
      "         44       0.00      0.00      0.00      1187\n",
      "        999       0.91      0.73      0.81      8444\n",
      "\n",
      "avg / total       0.53      0.56      0.50     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 9 214\n",
      "4 5 260\n",
      "5 39 720\n",
      "6 8 488\n",
      "7 39 1713\n",
      "8 9 1426\n",
      "9 8 1226\n",
      "12 39 163\n",
      "14 3 2\n",
      "15 39 415\n",
      "18 39 234\n",
      "19 9 191\n",
      "20 39 157\n",
      "21 39 354\n",
      "22 9 404\n",
      "23 9 93\n",
      "24 39 736\n",
      "25 39 789\n",
      "26 39 223\n",
      "27 39 324\n",
      "28 39 207\n",
      "29 39 157\n",
      "30 9 360\n",
      "31 39 87\n",
      "32 39 443\n",
      "33 39 928\n",
      "34 39 284\n",
      "35 39 1529\n",
      "36 39 1213\n",
      "37 39 1360\n",
      "38 39 2196\n",
      "39 40 290\n",
      "40 39 416\n",
      "41 39 280\n",
      "42 39 781\n",
      "43 39 587\n",
      "44 39 628\n",
      "999 3 727\n"
     ]
    }
   ],
   "source": [
    "# Random Forest unscaled features\n",
    "clf = random_forest(train_X,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.936\n",
      "F1 score: 0.491\n",
      "\n",
      "Clasification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.78      0.84      0.81      3643\n",
      "          4       0.44      0.02      0.04       346\n",
      "          5       0.90      0.41      0.57      4593\n",
      "          6       0.82      0.39      0.53      1277\n",
      "          7       0.48      0.28      0.35      5752\n",
      "          8       0.49      0.89      0.63     12161\n",
      "          9       0.60      0.44      0.51      9464\n",
      "         12       0.84      0.30      0.44       269\n",
      "         14       1.00      1.00      1.00         4\n",
      "         15       0.77      0.09      0.16       978\n",
      "         18       0.71      0.16      0.26       549\n",
      "         19       0.54      0.17      0.26       375\n",
      "         20       0.85      0.14      0.24       637\n",
      "         21       0.74      0.14      0.24       641\n",
      "         22       0.54      0.15      0.24       928\n",
      "         23       0.50      0.27      0.35       139\n",
      "         24       0.63      0.04      0.07      2609\n",
      "         25       0.23      0.09      0.13      3698\n",
      "         26       0.66      0.19      0.30       503\n",
      "         27       0.75      0.18      0.29       785\n",
      "         28       0.84      0.15      0.26       492\n",
      "         29       0.66      0.19      0.29       433\n",
      "         30       0.78      0.09      0.16      1081\n",
      "         31       0.68      0.60      0.64       594\n",
      "         32       0.85      0.09      0.16      1984\n",
      "         33       0.51      0.09      0.16      1315\n",
      "         34       0.78      0.13      0.23       719\n",
      "         35       0.47      0.10      0.17      2030\n",
      "         36       0.31      0.20      0.24      3005\n",
      "         37       0.74      0.10      0.17      2788\n",
      "         38       0.65      0.14      0.23      2912\n",
      "         39       0.27      0.89      0.42      9896\n",
      "         40       0.65      0.91      0.76      6130\n",
      "         41       0.71      0.15      0.25       583\n",
      "         42       0.88      0.05      0.10      1858\n",
      "         43       0.81      0.06      0.12       872\n",
      "         44       0.96      0.07      0.13      1187\n",
      "        999       0.82      0.77      0.79      8444\n",
      "\n",
      "avg / total       0.60      0.49      0.45     95674\n",
      "\n",
      "\n",
      "Summary confusion matrix:\n",
      "3 8 310\n",
      "4 8 96\n",
      "5 8 1004\n",
      "6 8 314\n",
      "7 39 2092\n",
      "8 9 637\n",
      "9 8 4548\n",
      "12 39 131\n",
      "14 999 0\n",
      "15 39 558\n",
      "18 39 234\n",
      "19 9 104\n",
      "20 39 257\n",
      "21 39 351\n",
      "22 39 256\n",
      "23 8 42\n",
      "24 39 1371\n",
      "25 39 2352\n",
      "26 39 195\n",
      "27 39 377\n",
      "28 39 190\n",
      "29 39 138\n",
      "30 39 400\n",
      "31 39 74\n",
      "32 39 1084\n",
      "33 39 918\n",
      "34 39 397\n",
      "35 39 1356\n",
      "36 39 1899\n",
      "37 39 1530\n",
      "38 39 1743\n",
      "39 36 252\n",
      "40 39 503\n",
      "41 39 390\n",
      "42 39 1307\n",
      "43 39 721\n",
      "44 39 672\n",
      "999 3 676\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting scaled and TruncatedSVD features\n",
    "clf = gradient_boosting(train_X_scaled_dimr,train_y)\n",
    "#print_gridsearch(clf)\n",
    "predictions(clf,train_X_scaled_dimr,train_y,test_trip_types)\n",
    "test_probs = clf.predict_proba(test_X_scaled_dimr)\n",
    "write_probs_to_file(test_visit_numbers,test_trip_types,test_probs)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
